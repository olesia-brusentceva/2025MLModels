{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1a41cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47573926",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'C:/Users/OlesiaBrusentseva/PyProjects/2025MLModels/MLmodels2/lab1Data/train.csv' # Assuming train.csv is in the same directory\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55c56cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Haversine Distance Calculation ---\n",
    "# Function to calculate Haversine distance between two sets of lat/lon coordinates\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371 # Radius of Earth in kilometers\n",
    "\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "    return distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9c190a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 10656 outliers from trip_duration. New shape: (1447988, 11)\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Initial Data Cleaning and Outlier Handling ---\n",
    "df['trip_duration'] = pd.to_numeric(df['trip_duration'], errors='coerce')\n",
    "df.dropna(subset=['trip_duration'], inplace=True)\n",
    "\n",
    "initial_rows = df.shape[0]\n",
    "df = df[(df['trip_duration'] >= 60) & (df['trip_duration'] <= 3600 * 6)]\n",
    "print(f\"Removed {initial_rows - df.shape[0]} outliers from trip_duration. New shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b091a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-transformed 'trip_duration' to 'log_trip_duration'.\n"
     ]
    }
   ],
   "source": [
    "# Log transform trip_duration to handle its skewed distribution\n",
    "df['log_trip_duration'] = np.log1p(df['trip_duration'])\n",
    "print(\"Log-transformed 'trip_duration' to 'log_trip_duration'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afe57df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing feature engineering...\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Feature Engineering ---\n",
    "print(\"Performing feature engineering...\")\n",
    "# Convert datetime columns\n",
    "df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bf691cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract time-based features from pickup_datetime\n",
    "df['pickup_hour'] = df['pickup_datetime'].dt.hour\n",
    "df['pickup_day_of_week'] = df['pickup_datetime'].dt.dayofweek # Monday=0, Sunday=6\n",
    "df['pickup_month'] = df['pickup_datetime'].dt.month\n",
    "df['pickup_day_of_year'] = df['pickup_datetime'].dt.dayofyear\n",
    "df['pickup_weekday'] = df['pickup_day_of_week'].apply(lambda x: 1 if x < 5 else 0) # 1 for weekday, 0 for weekend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bd68a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated 'haversine_distance'.\n"
     ]
    }
   ],
   "source": [
    "# Calculate Haversine distance\n",
    "df['haversine_distance'] = df.apply(\n",
    "    lambda row: haversine_distance(\n",
    "        row['pickup_latitude'], row['pickup_longitude'],\n",
    "        row['dropoff_latitude'], row['dropoff_longitude']\n",
    "    ), axis=1\n",
    ")\n",
    "print(\"Calculated 'haversine_distance'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764c377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle 'store_and_fwd_flag' - convert 'Y'/'N' to 1/0\n",
    "df['store_and_fwd_flag'] = df['store_and_fwd_flag'].map({'Y': 1, 'N': 0}).fillna(0) # Fillna for any potential NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b96ccc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped original 'id', 'pickup_datetime', 'dropoff_datetime', 'trip_duration' columns.\n"
     ]
    }
   ],
   "source": [
    "# Drop original datetime columns and trip_duration\n",
    "df_processed = df.drop(columns=['id', 'pickup_datetime', 'dropoff_datetime', 'trip_duration'])\n",
    "print(\"Dropped original 'id', 'pickup_datetime', 'dropoff_datetime', 'trip_duration' columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2da060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Define Features and Target ---\n",
    "# Features to be used in the model\n",
    "numerical_features = [\n",
    "    'passenger_count',\n",
    "    'pickup_longitude', 'pickup_latitude',\n",
    "    'dropoff_longitude', 'dropoff_latitude',\n",
    "    'haversine_distance',\n",
    "    'pickup_hour', 'pickup_day_of_week', 'pickup_month', 'pickup_day_of_year'\n",
    "]\n",
    "categorical_features = [\n",
    "    'vendor_id',\n",
    "    'store_and_fwd_flag', # binary\n",
    "    'pickup_weekday' # binary\n",
    "]\n",
    "target = 'log_trip_duration'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "995c3dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all selected features exist in the dataframe\n",
    "for col in numerical_features + categorical_features:\n",
    "    if col not in df_processed.columns:\n",
    "        print(f\"Warning: Feature '{col}' not found in processed DataFrame. Please check feature engineering steps.\")\n",
    "\n",
    "X = df_processed[numerical_features + categorical_features]\n",
    "y = df_processed[target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c559834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical features scaled. Categorical features prepared for direct use.\n",
      "Preprocessed X shape: (1447988, 13)\n",
      "   passenger_count  pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
      "0        -0.506088         -0.120654         0.519149           0.126373   \n",
      "1        -0.506088         -0.096079        -0.379503          -0.367716   \n",
      "2        -0.506088         -0.076462         0.396839          -0.450677   \n",
      "3        -0.506088         -0.514607        -0.948338          -0.548997   \n",
      "4        -0.506088          0.007933         1.292340           0.008800   \n",
      "\n",
      "   dropoff_latitude  haversine_distance  pickup_hour  pickup_day_of_week  \\\n",
      "0          0.384921           -0.456214     0.529638           -1.560836   \n",
      "1         -0.579200           -0.384789    -2.127957            1.510127   \n",
      "2         -1.168711            0.680717    -0.408337           -1.049009   \n",
      "3         -1.262977           -0.459243     0.842297           -0.537182   \n",
      "4          0.858387           -0.528324    -0.095678            0.998300   \n",
      "\n",
      "   pickup_month  pickup_day_of_year  vendor_id  store_and_fwd_flag  \\\n",
      "0     -0.307248           -0.345695          2                   0   \n",
      "1      1.477333            1.399632          1                   0   \n",
      "2     -1.496968           -1.412284          2                   0   \n",
      "3      0.287612            0.100333          2                   0   \n",
      "4     -0.307248           -0.112985          2                   0   \n",
      "\n",
      "   pickup_weekday  \n",
      "0               1  \n",
      "1               0  \n",
      "2               1  \n",
      "3               1  \n",
      "4               0  \n"
     ]
    }
   ],
   "source": [
    "# --- 6. Preprocessing Pipelines for Numerical and Categorical Features ---\n",
    "# Numerical pipeline: just scaling\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Identify columns for scaling\n",
    "X_numerical = X[numerical_features]\n",
    "X_categorical = X[categorical_features]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_numerical_scaled = scaler.fit_transform(X_numerical)\n",
    "X_numerical_scaled_df = pd.DataFrame(X_numerical_scaled, columns=numerical_features, index=X.index)\n",
    "\n",
    "# Combine scaled numerical features with original categorical features\n",
    "# We'll convert categorical features to appropriate types for PyTorch embeddings later\n",
    "X_preprocessed = pd.concat([X_numerical_scaled_df, X_categorical], axis=1)\n",
    "\n",
    "print(\"Numerical features scaled. Categorical features prepared for direct use.\")\n",
    "print(f\"Preprocessed X shape: {X_preprocessed.shape}\")\n",
    "print(X_preprocessed.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d0be85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set shape (X_train): (1158390, 13), (y_train): (1158390,)\n",
      "Validation set shape (X_val): (289598, 13), (y_val): (289598,)\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Split Data into Training and Validation Sets ---\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_preprocessed, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set shape (X_train): {X_train.shape}, (y_train): {y_train.shape}\")\n",
    "print(f\"Validation set shape (X_val): {X_val.shape}, (y_val): {y_val.shape}\")\n",
    "\n",
    "# Save processed data for the transformer model\n",
    "# Using .values to convert to numpy arrays for PyTorch\n",
    "np.save('X_train_numerical.npy', X_train[numerical_features].values)\n",
    "np.save('X_val_numerical.npy', X_val[numerical_features].values)\n",
    "np.save('y_train.npy', y_train.values)\n",
    "np.save('y_val.npy', y_val.values)\n",
    "\n",
    "# For categorical features, we need to ensure they are integer encoded for embedding lookup.\n",
    "\n",
    "vendor_id_mapping = {id: i for i, id in enumerate(df['vendor_id'].unique())}\n",
    "df_processed['vendor_id_encoded'] = df_processed['vendor_id'].map(vendor_id_mapping)\n",
    "\n",
    "# Update X_preprocessed with the encoded vendor_id\n",
    "X_preprocessed['vendor_id'] = df_processed['vendor_id_encoded']\n",
    "\n",
    "# Re-split with the updated X_preprocessed\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_preprocessed, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54bd734c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique vendor_ids: 2\n",
      "\n",
      "Data preprocessing complete. Data saved as .npy files.\n",
      "Ready for Transformer model training.\n"
     ]
    }
   ],
   "source": [
    "# Save categorical features as numpy arrays\n",
    "np.save('X_train_categorical.npy', X_train[categorical_features].values)\n",
    "np.save('X_val_categorical.npy', X_val[categorical_features].values)\n",
    "\n",
    "# Save the mapping for vendor_id and unique counts for embedding layers\n",
    "np.save('vendor_id_mapping.npy', np.array(list(vendor_id_mapping.items()), dtype=object))\n",
    "print(f\"Unique vendor_ids: {len(vendor_id_mapping)}\")\n",
    "\n",
    "print(\"\\nData preprocessing complete. Data saved as .npy files.\")\n",
    "print(\"Ready for Transformer model training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e4f290",
   "metadata": {},
   "source": [
    "Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e40d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 20 \n",
    "EMBEDDING_DIM = 32 # Dimension for categorical embeddings\n",
    "D_MODEL = 128 # Dimension of the transformer's input/output features\n",
    "N_HEAD = 4 # Number of attention heads\n",
    "NUM_ENCODER_LAYERS = 2 # Number of transformer encoder layers\n",
    "DROPOUT = 0.1\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92f24886",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. Custom Dataset Class ---\n",
    "class NYCTaxiDataset(Dataset):\n",
    "    def __init__(self, numerical_data, categorical_data, targets):\n",
    "        self.numerical_data = torch.tensor(numerical_data, dtype=torch.float32)\n",
    "        self.categorical_data = torch.tensor(categorical_data, dtype=torch.long)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32).unsqueeze(1) # Add a dimension for regression target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.numerical_data[idx], self.categorical_data[idx], self.targets[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784ce20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 2. Transformer Model Definition ---\n",
    "class TaxiTransformer(nn.Module):\n",
    "    def __init__(self, num_numerical_features, num_vendor_ids, embedding_dim,\n",
    "                 d_model, n_head, num_encoder_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Embedding for vendor_id\n",
    "        self.vendor_embedding = nn.Embedding(num_vendor_ids, embedding_dim)\n",
    "\n",
    "        # Linear layer to project numerical features to d_model space\n",
    "    \n",
    "        self.numerical_projection = nn.Linear(num_numerical_features, d_model - (embedding_dim * 3))\n",
    "\n",
    "        # Linear layers for store_and_fwd_flag and pickup_weekday (binary, treated as categorical)\n",
    "        \n",
    "        self.store_flag_embedding = nn.Embedding(2, embedding_dim) # 2 classes: 0 or 1\n",
    "        self.weekday_embedding = nn.Embedding(2, embedding_dim) # 2 classes: 0 or 1\n",
    "\n",
    "        # Transformer Encoder Layer\n",
    "        # The transformer will process a single combined feature vector per trip.\n",
    "        # The self-attention mechanism will learn interactions between the components of this vector.\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_head,\n",
    "            dim_feedforward=d_model * 4, # Standard practice\n",
    "            dropout=dropout,\n",
    "            batch_first=True # Input and output tensors are (batch, sequence, feature)\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "\n",
    "        # Output regression head\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, 1) # Output a single regression value\n",
    "        )\n",
    "\n",
    "    def forward(self, numerical_features, categorical_features):\n",
    "        # Unpack categorical features\n",
    "        vendor_ids = categorical_features[:, 0] # vendor_id is the first categorical feature\n",
    "        store_flags = categorical_features[:, 1] # store_and_fwd_flag is the second\n",
    "        weekdays = categorical_features[:, 2] # pickup_weekday is the third\n",
    "\n",
    "        # Embed categorical features\n",
    "        vendor_embed = self.vendor_embedding(vendor_ids)\n",
    "        store_flag_embed = self.store_flag_embedding(store_flags)\n",
    "        weekday_embed = self.weekday_embedding(weekdays)\n",
    "\n",
    "        # Project numerical features\n",
    "        numerical_proj = self.numerical_projection(numerical_features)\n",
    "\n",
    "        # Concatenate all features to form the combined input vector for the transformer\n",
    "        # The sequence length is 1, as each sample is a single combined vector\n",
    "        combined_features = torch.cat((numerical_proj, vendor_embed, store_flag_embed, weekday_embed), dim=1)\n",
    "        \n",
    "        # Add a sequence dimension (batch_size, 1, d_model) for the transformer\n",
    "        combined_features = combined_features.unsqueeze(1)\n",
    "\n",
    "        # Pass through transformer encoder\n",
    "        transformer_output = self.transformer_encoder(combined_features)\n",
    "\n",
    "        # Take the output for the first (and only) token in the sequence\n",
    "        # and pass it to the regressor\n",
    "        output = self.regressor(transformer_output.squeeze(1)) # Remove the sequence dimension\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2cd17698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data...\n",
      "Preprocessed data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 3. Load Preprocessed Data ---\n",
    "print(\"Loading preprocessed data...\")\n",
    "try:\n",
    "    X_train_numerical = np.load('X_train_numerical.npy')\n",
    "    X_val_numerical = np.load('X_val_numerical.npy')\n",
    "    X_train_categorical = np.load('X_train_categorical.npy')\n",
    "    X_val_categorical = np.load('X_val_categorical.npy')\n",
    "    y_train = np.load('y_train.npy')\n",
    "    y_val = np.load('y_val.npy')\n",
    "    vendor_id_mapping_items = np.load('vendor_id_mapping.npy', allow_pickle=True)\n",
    "    num_vendor_ids = len(vendor_id_mapping_items)\n",
    "    print(\"Preprocessed data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Preprocessed .npy files not found. Please run the data_preprocessing_normalization.py script first.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6a833cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model initialized:\n",
      "TaxiTransformer(\n",
      "  (vendor_embedding): Embedding(2, 32)\n",
      "  (numerical_projection): Linear(in_features=10, out_features=32, bias=True)\n",
      "  (store_flag_embedding): Embedding(2, 32)\n",
      "  (weekday_embedding): Embedding(2, 32)\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (regressor): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters: 405409\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create Dataset and DataLoader instances\n",
    "train_dataset = NYCTaxiDataset(X_train_numerical, X_train_categorical, y_train)\n",
    "val_dataset = NYCTaxiDataset(X_val_numerical, X_val_categorical, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# --- 4. Initialize Model, Loss, and Optimizer ---\n",
    "num_numerical_features = X_train_numerical.shape[1]\n",
    "\n",
    "model = TaxiTransformer(\n",
    "    num_numerical_features=num_numerical_features,\n",
    "    num_vendor_ids=num_vendor_ids,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    d_model=D_MODEL,\n",
    "    n_head=N_HEAD,\n",
    "    num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(DEVICE)\n",
    "\n",
    "criterion = nn.MSELoss() # Mean Squared Error for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f\"\\nModel initialized:\\n{model}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7a161a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Epoch [1/20], Train Loss: 0.3912, Val Loss: 0.1512\n",
      "Saved best model with Val Loss: 0.1512\n",
      "Epoch [2/20], Train Loss: 0.2574, Val Loss: 0.1276\n",
      "Saved best model with Val Loss: 0.1276\n",
      "Epoch [3/20], Train Loss: 0.2262, Val Loss: 0.1190\n",
      "Saved best model with Val Loss: 0.1190\n",
      "Epoch [4/20], Train Loss: 0.2062, Val Loss: 0.1157\n",
      "Saved best model with Val Loss: 0.1157\n",
      "Epoch [5/20], Train Loss: 0.1904, Val Loss: 0.1168\n",
      "Epoch [6/20], Train Loss: 0.1760, Val Loss: 0.1231\n",
      "Epoch [7/20], Train Loss: 0.1640, Val Loss: 0.1116\n",
      "Saved best model with Val Loss: 0.1116\n",
      "Epoch [8/20], Train Loss: 0.1546, Val Loss: 0.1100\n",
      "Saved best model with Val Loss: 0.1100\n",
      "Epoch [9/20], Train Loss: 0.1463, Val Loss: 0.1109\n",
      "Epoch [10/20], Train Loss: 0.1392, Val Loss: 0.1074\n",
      "Saved best model with Val Loss: 0.1074\n",
      "Epoch [11/20], Train Loss: 0.1337, Val Loss: 0.1082\n",
      "Epoch [12/20], Train Loss: 0.1288, Val Loss: 0.1053\n",
      "Saved best model with Val Loss: 0.1053\n",
      "Epoch [13/20], Train Loss: 0.1246, Val Loss: 0.1060\n",
      "Epoch [14/20], Train Loss: 0.1212, Val Loss: 0.1070\n",
      "Epoch [15/20], Train Loss: 0.1185, Val Loss: 0.1042\n",
      "Saved best model with Val Loss: 0.1042\n",
      "Epoch [16/20], Train Loss: 0.1160, Val Loss: 0.1042\n",
      "Epoch [17/20], Train Loss: 0.1142, Val Loss: 0.1045\n",
      "Epoch [18/20], Train Loss: 0.1126, Val Loss: 0.1041\n",
      "Saved best model with Val Loss: 0.1041\n",
      "Epoch [19/20], Train Loss: 0.1113, Val Loss: 0.1033\n",
      "Saved best model with Val Loss: 0.1033\n",
      "Epoch [20/20], Train Loss: 0.1103, Val Loss: 0.1040\n",
      "\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 5. Training Loop ---\n",
    "print(\"\\nStarting training...\")\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (numerical_data, categorical_data, targets) in enumerate(train_loader):\n",
    "        numerical_data, categorical_data, targets = numerical_data.to(DEVICE), categorical_data.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(numerical_data, categorical_data)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for numerical_data, categorical_data, targets in val_loader:\n",
    "            numerical_data, categorical_data, targets = numerical_data.to(DEVICE), categorical_data.to(DEVICE), targets.to(DEVICE)\n",
    "            outputs = model(numerical_data, categorical_data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Save the best model based on validation loss\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'best_taxi_transformer_model.pth')\n",
    "        print(f\"Saved best model with Val Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5132a49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating best model on validation set...\n",
      "Final Validation MSE: 0.1033\n",
      "Final Validation RMSE (log-transformed): 0.3214\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 6. Evaluation (Load best model and evaluate on validation set) ---\n",
    "print(\"\\nEvaluating best model on validation set...\")\n",
    "model.load_state_dict(torch.load('best_taxi_transformer_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "total_val_loss = 0\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for numerical_data, categorical_data, targets in val_loader:\n",
    "        numerical_data, categorical_data, targets = numerical_data.to(DEVICE), categorical_data.to(DEVICE), targets.to(DEVICE)\n",
    "        outputs = model(numerical_data, categorical_data)\n",
    "        loss = criterion(outputs, targets)\n",
    "        total_val_loss += loss.item()\n",
    "        all_preds.extend(outputs.cpu().numpy())\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "final_val_loss = total_val_loss / len(val_loader)\n",
    "print(f\"Final Validation MSE: {final_val_loss:.4f}\")\n",
    "\n",
    "# Calculate RMSE (Root Mean Squared Error)\n",
    "rmse = np.sqrt(final_val_loss)\n",
    "print(f\"Final Validation RMSE (log-transformed): {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc5db65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate Final Validation RMSE (original scale): 306.09 seconds\n",
      "\n",
      "Example prediction:\n",
      "Predicted log duration: 6.5311\n",
      "Actual log duration: 6.6503\n",
      "Predicted duration (seconds): 685.17\n",
      "Actual duration (seconds): 772.00\n"
     ]
    }
   ],
   "source": [
    "all_preds_original_scale = np.expm1(np.array(all_preds))\n",
    "all_targets_original_scale = np.expm1(np.array(all_targets))\n",
    "\n",
    "rmse_original_scale = np.sqrt(np.mean((all_preds_original_scale - all_targets_original_scale)**2))\n",
    "print(f\"Approximate Final Validation RMSE (original scale): {rmse_original_scale:.2f} seconds\")\n",
    "\n",
    "# Example of a single prediction (using the first validation sample)\n",
    "print(\"\\nExample prediction:\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_numerical, sample_categorical, sample_target = val_dataset[0]\n",
    "    sample_numerical = sample_numerical.unsqueeze(0).to(DEVICE) # Add batch dimension\n",
    "    sample_categorical = sample_categorical.unsqueeze(0).to(DEVICE) # Add batch dimension\n",
    "\n",
    "    predicted_log_duration = model(sample_numerical, sample_categorical).item()\n",
    "    predicted_duration = np.expm1(predicted_log_duration)\n",
    "    actual_duration = np.expm1(sample_target.item())\n",
    "\n",
    "    print(f\"Predicted log duration: {predicted_log_duration:.4f}\")\n",
    "    print(f\"Actual log duration: {sample_target.item():.4f}\")\n",
    "    print(f\"Predicted duration (seconds): {predicted_duration:.2f}\")\n",
    "    print(f\"Actual duration (seconds): {actual_duration:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5519910f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
